{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook0ab4ce142d",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install wget"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-07T22:10:07.714869Z",
          "iopub.execute_input": "2024-05-07T22:10:07.715743Z",
          "iopub.status.idle": "2024-05-07T22:10:25.468281Z",
          "shell.execute_reply.started": "2024-05-07T22:10:07.71571Z",
          "shell.execute_reply": "2024-05-07T22:10:25.467099Z"
        },
        "trusted": true,
        "id": "mbJu72uwxCUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T22:10:38.790887Z",
          "iopub.execute_input": "2024-05-07T22:10:38.791271Z",
          "iopub.status.idle": "2024-05-07T22:10:38.800214Z",
          "shell.execute_reply.started": "2024-05-07T22:10:38.791224Z",
          "shell.execute_reply": "2024-05-07T22:10:38.799273Z"
        },
        "trusted": true,
        "id": "s-ajtLeyxCUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://files.icg.tugraz.at/f/b0623306eb9246be8c3c/?dl=1\" -O dataset.zip\n",
        "!unzip dataset.zip -d /kaggle/working/completer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T22:10:47.34425Z",
          "iopub.execute_input": "2024-05-07T22:10:47.344903Z",
          "iopub.status.idle": "2024-05-07T22:11:06.149446Z",
          "shell.execute_reply.started": "2024-05-07T22:10:47.344872Z",
          "shell.execute_reply": "2024-05-07T22:11:06.148154Z"
        },
        "trusted": true,
        "id": "XMHNdCgExCUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import SimpleITK as sitk\n",
        "import nibabel\n",
        "import nibabel.processing\n",
        "import argparse\n",
        "from scipy.ndimage import zoom\n",
        "tf.disable_v2_behavior()\n",
        "import os\n",
        "import csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T22:12:22.978863Z",
          "iopub.execute_input": "2024-05-07T22:12:22.979914Z",
          "iopub.status.idle": "2024-05-07T22:12:22.985987Z",
          "shell.execute_reply.started": "2024-05-07T22:12:22.979876Z",
          "shell.execute_reply": "2024-05-07T22:12:22.984739Z"
        },
        "trusted": true,
        "id": "dUUlj4fzxCUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if \"GPU\" not in device_name:\n",
        "    print(\"GPU device not found\")\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T22:12:32.107435Z",
          "iopub.execute_input": "2024-05-07T22:12:32.1078Z",
          "iopub.status.idle": "2024-05-07T22:12:32.534235Z",
          "shell.execute_reply.started": "2024-05-07T22:12:32.107773Z",
          "shell.execute_reply": "2024-05-07T22:12:32.532794Z"
        },
        "trusted": true,
        "id": "eleaBa-1xCUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class auto_encoder(object):\n",
        "    def __init__(self, sess):\n",
        "        self.sess           = sess\n",
        "        self.phase          = 'train'\n",
        "        self.batch_size     = 1\n",
        "        self.inputI_size    = 128\n",
        "        self.inputI_chn     = 1\n",
        "        self.output_chn     = 12\n",
        "        self.lr             = 0.0001\n",
        "        self.beta1          = 0.3\n",
        "        self.epoch          = 50\n",
        "        self.model_name     = 'n1.model'\n",
        "        self.save_intval    = 10\n",
        "        self.build_model()\n",
        "        self.chkpoint_dir   = \"/kaggle/working/ckpt2\"\n",
        "        self.train_data_dir = \"/kaggle/working/completer/dataset/test/incomplete/\"\n",
        "        self.train_label_dir = \"/kaggle/working/completer/dataset/test/complete\"\n",
        "        self.test_data_dir = \"./real_test/incomplete\"\n",
        "\n",
        "        self.test_label_dir=\"./Dtest4/dataset/0_ground_truth/lung/\"\n",
        "        self.save_output_dir = \"./output_multiclass/\"\n",
        "        self.save_residual_dir = \"./output_multiclass/residual/\"\n",
        "\n",
        "\n",
        "    def dice_loss_fun(self, pred, input_gt):\n",
        "        input_gt = tf.one_hot(input_gt, 12)\n",
        "        dice = 0\n",
        "        for i in range(12):\n",
        "            inse = tf.reduce_mean(pred[:, :, :, :, i]*input_gt[:, :, :, :, i])\n",
        "            l = tf.reduce_sum(pred[:, :, :, :, i]*pred[:, :, :, :, i])\n",
        "            r = tf.reduce_sum(input_gt[:, :, :, :, i] * input_gt[:, :, :, :, i])\n",
        "            dice = dice + 2*inse/(l+r)\n",
        "        return -dice\n",
        "\n",
        "\n",
        "\n",
        "    def conv3d(self,input, output_chn, kernel_size, stride, use_bias=False, name='conv'):\n",
        "\n",
        "        filter_shape = [kernel_size, kernel_size, kernel_size, input.shape[4], output_chn]\n",
        "        filters = tf.Variable(tf.random.normal(filter_shape))\n",
        "        return tf.nn.conv3d(input=input, filters=filters, strides=[1,stride,stride,stride,1],\n",
        "                                padding='SAME', data_format=\"NDHWC\",name=name)\n",
        "\n",
        "\n",
        "    def conv_bn_relu(self,input, output_chn, kernel_size, stride, use_bias, is_training, name):\n",
        "        with tf.variable_scope(name):\n",
        "            conv = self.conv3d(input, output_chn, kernel_size, stride, use_bias, name='conv')\n",
        "            relu = tf.nn.relu(conv, name='relu')\n",
        "        return relu\n",
        "\n",
        "\n",
        "\n",
        "    def Deconv3d(self,input, output_chn, name):\n",
        "        batch, in_depth, in_height, in_width, in_channels = [int(d) for d in input.get_shape()]\n",
        "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "            filter = tf.get_variable(name+\"/filter\", shape=[4, 4, 4, output_chn, in_channels], dtype=tf.float32,\n",
        "                                     initializer=tf.random_normal_initializer(0, 0.01))\n",
        "            conv = tf.nn.conv3d_transpose(value=input, filter=filter, output_shape=[batch, in_depth * 2, in_height * 2, in_width * 2, output_chn],\n",
        "                                          strides=[1, 2, 2, 2, 1], padding=\"SAME\", name=name)\n",
        "        return conv\n",
        "\n",
        "\n",
        "\n",
        "    def deconv_bn_relu(self,input, output_chn, is_training, name):\n",
        "        with tf.variable_scope(name):\n",
        "            conv = self.Deconv3d(input, output_chn, name='deconv')\n",
        "            relu = tf.nn.relu(conv, name='relu')\n",
        "        return relu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        print('building patch based model...')\n",
        "        self.input_I = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.inputI_size,self.inputI_size,128, self.inputI_chn], name='inputI')\n",
        "        self.input_gt = tf.placeholder(dtype=tf.int64, shape=[self.batch_size,self.inputI_size,self.inputI_size,128,1], name='target')\n",
        "        self.soft_prob , self.task0_label = self.encoder_decoder(self.input_I)\n",
        "        self.main_dice_loss = self.dice_loss_fun(self.soft_prob, self.input_gt[:,:,:,:,0])\n",
        "        self.dice_loss=200000000*self.main_dice_loss\n",
        "        self.Loss = self.dice_loss\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "    def encoder_decoder(self, inputI):\n",
        "        phase_flag = (self.phase=='train')\n",
        "        conv1_1 = self.conv3d(input=inputI, output_chn=64, kernel_size=3, stride=2, use_bias=True, name='conv1')\n",
        "        conv1_relu = tf.nn.relu(conv1_1, name='conv1_relu')\n",
        "        conv2_1 = self.conv3d(input=conv1_relu, output_chn=128, kernel_size=3, stride=2, use_bias=True, name='conv2')\n",
        "        conv2_relu = tf.nn.relu(conv2_1, name='conv2_relu')\n",
        "        conv3_1 = self.conv3d(input=conv2_relu, output_chn= 256, kernel_size=3, stride=2, use_bias=True, name='conv3a')\n",
        "        conv3_relu = tf.nn.relu(conv3_1, name='conv3_1_relu')\n",
        "        conv4_1 = self.conv3d(input=conv3_relu, output_chn=512, kernel_size=3, stride=2, use_bias=True, name='conv4a')\n",
        "        conv4_relu = tf.nn.relu(conv4_1, name='conv4_1_relu')\n",
        "        conv5_1 = self.conv3d(input=conv4_relu, output_chn=512, kernel_size=3, stride=1, use_bias=True, name='conv5a')\n",
        "        conv5_relu = tf.nn.relu(conv5_1, name='conv5_1_relu')\n",
        "        feature= self.conv_bn_relu(input=conv5_relu, output_chn=256, kernel_size=3, stride=1, use_bias=True, is_training=phase_flag, name='conv6_1')\n",
        "        deconv1_1_input = tf.concat([conv4_relu, feature], axis=4)\n",
        "        deconv1_1 = self.deconv_bn_relu(input=deconv1_1_input, output_chn=256, is_training=phase_flag, name='deconv1_1')\n",
        "        deconv1_2 = self.conv_bn_relu(input=deconv1_1, output_chn=128, kernel_size=3, stride=1, use_bias=True, is_training=phase_flag, name='deconv1_2')\n",
        "        deconv2_1_input = tf.concat([conv3_relu, deconv1_2], axis=4)\n",
        "        deconv2_1 = self.deconv_bn_relu(input=deconv2_1_input, output_chn=128, is_training=phase_flag, name='deconv2_1')\n",
        "        deconv2_2 = self.conv_bn_relu(input=deconv2_1, output_chn=64, kernel_size=3,stride=1, use_bias=True, is_training=phase_flag, name='deconv2_2')\n",
        "        deconv3_1_input = tf.concat([conv2_relu, deconv2_2], axis=4)\n",
        "        deconv3_1 = self.deconv_bn_relu(input=deconv3_1_input, output_chn=64, is_training=phase_flag, name='deconv3_1')\n",
        "        deconv3_2 = self.conv_bn_relu(input=deconv3_1, output_chn=64, kernel_size=3, stride=1, use_bias=True, is_training=phase_flag, name='deconv3_2')\n",
        "        deconv4_1 = self.deconv_bn_relu(input=deconv3_2, output_chn=32, is_training=phase_flag, name='deconv4_1')\n",
        "        deconv4_2 = self.conv_bn_relu(input=deconv4_1, output_chn=32, kernel_size=3, stride=1, use_bias=True, is_training=phase_flag, name='deconv4_2')\n",
        "        pred_prob1 = self.conv_bn_relu(input=deconv4_2, output_chn=self.output_chn, kernel_size=3, stride=1, use_bias=True, is_training=phase_flag, name='pred_prob1')\n",
        "        pred_prob = self.conv3d(input=pred_prob1, output_chn=self.output_chn, kernel_size=3, stride=1, use_bias=True, name='pred_prob')\n",
        "        pred_prob2 = self.conv3d(input=pred_prob, output_chn=self.output_chn, kernel_size=3, stride=1, use_bias=True, name='pred_prob2')\n",
        "        pred_prob3 = self.conv3d(input=pred_prob2, output_chn=self.output_chn, kernel_size=3, stride=1, use_bias=True, name='pred_prob3')\n",
        "        soft_prob=tf.nn.softmax(pred_prob3,name='task_0')\n",
        "        task0_label=tf.argmax(soft_prob,axis=4,name='argmax0')\n",
        "        return  soft_prob,task0_label\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        u_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=self.beta1).minimize(self.Loss)\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        self.sess.run(init_op)\n",
        "        counter = 1\n",
        "\n",
        "        train_label_list = glob('{}/*.nii.gz'.format(self.train_label_dir))\n",
        "        i = 0\n",
        "#         print(train_label_list)\n",
        "        csv_file_path = '/kaggle/working/losses.csv'\n",
        "\n",
        "        # Check if CSV file exists, create it if it doesn't\n",
        "        if not os.path.isfile(csv_file_path):\n",
        "#             print(\"here\")\n",
        "            with open(csv_file_path, 'w', newline='') as csvfile:\n",
        "                csvwriter = csv.writer(csvfile)\n",
        "                csvwriter.writerow(['Epoch', 'Iteration', 'Loss'])  # Write header\n",
        "\n",
        "        with open(csv_file_path, 'a', newline='') as csvfile:\n",
        "            csvwriter = csv.writer(csvfile)\n",
        "\n",
        "            for epoch in range(self.epoch):\n",
        "                epoch_losses = []  # List to collect losses for the current epoch\n",
        "                print('Epoch:', epoch + 1)\n",
        "\n",
        "                for j in range(len(train_label_list)):\n",
        "                    labelImg = sitk.ReadImage(train_label_list[j])\n",
        "                    labelNpy = sitk.GetArrayFromImage(labelImg)\n",
        "                    labelNpy_resized = zoom(labelNpy, (128/labelNpy.shape[0], 128/labelNpy.shape[1], 128/labelNpy.shape[2]), order=0, mode='constant')\n",
        "                    labelNpy_resized = np.expand_dims(np.expand_dims(labelNpy_resized, axis=0), axis=4)\n",
        "                    name = train_label_list[j][-len('_full.nii.gz')-len('s0556'):-len('_full.nii.gz')]\n",
        "\n",
        "                    for k in range(10):\n",
        "                        data_dir = self.train_data_dir + str(name) + '/' + str(name) + '_%d' % k + '.nii.gz'\n",
        "                        trainImg = sitk.ReadImage(data_dir)\n",
        "                        trainNpy = sitk.GetArrayFromImage(trainImg)\n",
        "                        trainNpy_resized = zoom(trainNpy, (128/trainNpy.shape[0], 128/trainNpy.shape[1], 128/trainNpy.shape[2]), order=0, mode='constant')\n",
        "                        trainNpy_resized = np.expand_dims(np.expand_dims(trainNpy_resized, axis=0), axis=4)\n",
        "\n",
        "                        _, cur_train_loss = self.sess.run([u_optimizer, self.Loss], feed_dict={self.input_I: trainNpy_resized, self.input_gt: labelNpy_resized})\n",
        "\n",
        "                        # Append epoch, iteration, and loss to epoch_losses\n",
        "                        epoch_losses.append((epoch + 1, (j * 10) + k + 1, cur_train_loss))\n",
        "#                         print(len(epoch_losses))\n",
        "#                         csvwriter.writerow(epoch_losses[-1])\n",
        "                        train_output0 = self.sess.run(self.task0_label, feed_dict={self.input_I: trainNpy_resized})\n",
        "#                         print('Sum for current training whole: %.8f, Pred whole:  %.8f' % (np.sum(labelNpy_resized), np.sum(train_output0)))\n",
        "                        print('Current training loss:', cur_train_loss)\n",
        "                    print(\"......\",j,\".....\")\n",
        "\n",
        "#                 Write epoch_losses to CSV file after each epoch\n",
        "                for loss_entry in epoch_losses:\n",
        "                    csvwriter.writerow(loss_entry)\n",
        "\n",
        "                counter += 1\n",
        "                if np.mod(counter, self.save_intval) == 0:\n",
        "                    self.save_chkpoint(self.chkpoint_dir, self.model_name, counter)\n",
        "\n",
        "        # Save final checkpoint after training completes\n",
        "        self.save_chkpoint(self.chkpoint_dir, self.model_name, counter)\n",
        "\n",
        "    def test(self):\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        self.sess.run(init_op)\n",
        "        if self.load_chkpoint(self.chkpoint_dir):\n",
        "            print(\" **Successfully load the checkpoint***\")\n",
        "        else:\n",
        "            print(\"**Fail to load the checkpoint******\")\n",
        "\n",
        "        test_list=glob('{}/*.nii.gz'.format(self.test_data_dir))\n",
        "\n",
        "        k=1\n",
        "        for i in range(len(test_list)):\n",
        "\n",
        "            ### input\n",
        "            print(test_list[i])\n",
        "            test_img=sitk.ReadImage(test_list[i])\n",
        "            test_input = sitk.GetArrayFromImage(test_img)\n",
        "            test_input_resized_ = zoom(test_input,(256/test_input.shape[0],256/test_input.shape[1],128/test_input.shape[2]),order=0, mode='constant')\n",
        "            test_input_resized_[test_input_resized_>12]=0\n",
        "            test_input_resized_[test_input_resized_<0]=0\n",
        "            print('test_input_resized_',np.unique(test_input_resized_))\n",
        "            test_input_resized=np.expand_dims(np.expand_dims(test_input_resized_,axis=0),axis=4)\n",
        "\n",
        "\n",
        "            ## prediction\n",
        "            test_output = self.sess.run(self.task0_label, feed_dict={self.input_I: test_input_resized})\n",
        "            print(test_output.shape)\n",
        "            print(np.unique(test_output))\n",
        "\n",
        "\n",
        "            ## output\n",
        "            filename=self.save_output_dir+test_list[i][-7-len('s0332_0'):-7]+'.nii.gz'\n",
        "            resize2original=1\n",
        "\n",
        "            if resize2original:\n",
        "                print('resizing predictions...')\n",
        "\n",
        "                test_output=zoom(test_output[0],(test_input.shape[0]/256,test_input.shape[1]/256,test_input.shape[2]/128),order=0, mode='constant')\n",
        "                print(test_output.shape)\n",
        "\n",
        "                test_output[test_output>12]=0\n",
        "                test_output[test_output<0]=0\n",
        "                test_pred=sitk.GetImageFromArray(test_output.astype('int32'))\n",
        "                test_pred.CopyInformation(test_img)\n",
        "                sitk.WriteImage(test_pred,filename)\n",
        "\n",
        "            else:\n",
        "                print('resizing input...')\n",
        "                #test_img_downsampled=self.downsamplePatient(test_img,test_input.shape[0]/256,test_input.shape[1]/256,test_input.shape[2]/128)\n",
        "                print('resizing done...')\n",
        "\n",
        "                input_img = nibabel.load(test_list[i])\n",
        "\n",
        "                voxel_size=input_img.header.get_zooms()\n",
        "                voxel_size_new=[voxel_size[0](test_input.shape[0]/256),voxel_size[1](test_input.shape[1]/256),voxel_size[2]*(test_input.shape[2]/128)]\n",
        "                resampled_img = nibabel.processing.resample_to_output(input_img, voxel_size_new)\n",
        "                filename_img=self.save_output_dir+test_list[i][-7-len('s0332_0'):-7]+'_org'+'.nii.gz'\n",
        "                nibabel.save(resampled_img, filename_img)\n",
        "\n",
        "\n",
        "                test_pred=sitk.GetImageFromArray(test_output[0].astype('int32'))\n",
        "                sitk.WriteImage(test_pred,filename)\n",
        "\n",
        "\n",
        "            k+=1\n",
        "            #filename_res=self.save_residual_dir+test_list[i][-7-len('s0332_0'):-7]+'.nii.gz'\n",
        "            #res_output=test_output-test_input\n",
        "            #res_output_img=sitk.GetImageFromArray(res_output.astype('int32'))\n",
        "            #res_output_img.CopyInformation(test_img)\n",
        "            #sitk.WriteImage(res_output_img,filename_res)\n",
        "\n",
        "\n",
        "\n",
        "    def save_chkpoint(self, checkpoint_dir, model_name, step):\n",
        "        model_dir = \"%s\" % ('ckpt')\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
        "\n",
        "\n",
        "\n",
        "    def load_chkpoint(self, checkpoint_dir):\n",
        "        print(\" [*] Reading checkpoint...\")\n",
        "        model_dir = \"%s\" % ('ckpt')\n",
        "        print('########################################################')\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "# if _name_ == \"_main_\":\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\"--phase\")\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "\n",
        "#     if args.phase == \"test\":\n",
        "#         print('testing model...')\n",
        "#         model.test()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\"--phase\")\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True  # Allow GPU memory growth\n",
        "    config.gpu_options.visible_device_list = \"0\"  # Specify GPU device\n",
        "    sess1 = tf.compat.v1.Session(config=config)\n",
        "    with sess1.as_default():\n",
        "        with sess1.graph.as_default():\n",
        "            model = auto_encoder(sess1)\n",
        "            total_parameters = 0\n",
        "            for variable in tf.trainable_variables():\n",
        "                shape = variable.get_shape()\n",
        "                variable_parameters = 1\n",
        "                for dim in shape:\n",
        "                    variable_parameters *= dim.value\n",
        "                total_parameters += variable_parameters\n",
        "            print('trainable params:',total_parameters)\n",
        "\n",
        "#     if args.phase == \"train\":\n",
        "    print('training model...')\n",
        "    model.train()\n",
        "#     if args.phase == \"test\":\n",
        "#         print('testing model...')\n",
        "#         model.test()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T22:12:40.53088Z",
          "iopub.execute_input": "2024-05-07T22:12:40.531344Z"
        },
        "trusted": true,
        "id": "e6eXdKldxCUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wilsMNKlxCUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}